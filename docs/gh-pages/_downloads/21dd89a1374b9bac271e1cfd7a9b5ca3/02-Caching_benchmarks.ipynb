{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Caching benchmarks\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Saving benchmarks\n\nPerformance benchmarks as run by pyquickbench are typically quite lengthy. \nWhile obvious for complex benchmarks, this is still true for simple benchmarks. \nIndeed, by default, pyquickbench uses the standard :meth:`python:timeit.Timer.autorange` to assess the number of times a benchmark should be run to ensure reliable timings, which comes with significant overhead as any call to this function will require a non-configurable minimum execution time of 0.2 seconds.\nCaching benchmarks results is a great way to reduce this overhead.\n\nIn pyquickbench, caching results in order to avoid a full re-run is as simple as providing a file name to :func:`pyquickbench.run_benchmark`. If this file does not exist, it will be created. Both ``*.npy`` and ``*.npz`` extensions are accepted.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\nimport pyquickbench\n\ndef comprehension(n):\n    return ['' for _ in range(n)]\n\ndef star_operator(n):\n    return ['']*n\n\ndef for_loop_append(n):\n    l = []\n    for _ in range(n):\n        l.append('')\n    \nall_funs = [\n    comprehension   ,\n    star_operator   ,\n    for_loop_append ,\n]\n \nn_bench = 12\nall_sizes = [2**n for n in range(n_bench)]\n\ntimings_filename = \"My_benchmark_file.npy\"\n\n\nall_times = pyquickbench.run_benchmark(\n    all_sizes                       ,\n    all_funs                        ,\n    filename = timings_filename     ,\n) \n\npyquickbench.plot_benchmark(\n    all_times   ,\n    all_sizes   ,\n    all_funs    ,\n    show = True ,\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Skipping benchmarks\n\nAnother call to :func:`pyquickbench.run_benchmark` will detect that the file exists. The benchmark will not be run the a second time, and the contents of the file is used instead.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "all_times_bis = pyquickbench.run_benchmark(\n    all_sizes                       ,\n    all_funs                        ,\n    filename = timings_filename     ,\n) \n\nnp.all(all_times == all_times_bis)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Forcing benchmarks\n\nA full re-run can nonetheless be forced if the keyword ``ForceBenchmark`` is set to ``True``. The default value for ``ForceBenchmark`` is ``False``.\n\n```\nall_times_ter = pyquickbench.run_benchmark(\n    all_sizes                      ,\n    all_funs                       ,\n    filename = timings_filename    ,\n    ForceBenchmark = True          ,\n)\n```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Detecting obselete benchmarks\n\nIf the file on disk corresponds to a benchmark with more or fewer runs, the whole benchmark is run again, and the contents of the file is updated. For instance, the following will run the benchmark again:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "n_bench = 8\nall_sizes_small = [2**n for n in range(n_bench)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "all_times_small = pyquickbench.run_benchmark(\n    all_sizes_small                 ,\n    all_funs                        ,\n    filename = timings_filename     ,\n) \n\npyquickbench.plot_benchmark(\n    all_times_small ,\n    all_sizes_small ,\n    all_funs        ,\n    show = True     ,\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can check that the two benchmarks have non-matching shapes.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(f'Initial benchmark shape {all_times.shape}')\nprint(f'Current benchmark shape {all_times_small.shape}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This mechanism can easily be tricked as it only checks for array dimensions and not content. Indeed, ``all_sizes`` is not stored in the ``*.npy`` benchmark file, and the test only relies on file size.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "all_sizes_lin = [1+n for n in range(n_bench)]\nprint(len(all_sizes_lin) == len(all_sizes_small))\nprint(all([size_lin == size_small for size_lin, size_small in zip(all_sizes_lin, all_sizes_small)]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Hence, the following will not trigger a new benchmark although it should, as the sizes are not stored in the ``*.npy`` benchmark file. The following plot is inaccurate.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "pyquickbench.run_benchmark(\n    all_sizes_lin                   ,\n    all_funs                        ,\n    filename = timings_filename     ,\n    show = True                     ,\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Using a ``*.npz`` file as an output allows :func:`pyquickbench.run_benchmark` to detect this change. For instance, both outputs here are correct.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "timings_filename = \"My_benchmark_file.npz\"\n\n\npyquickbench.run_benchmark(\n    all_sizes_small                 ,\n    all_funs                        ,\n    filename = timings_filename     ,\n    show = True                     ,\n)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "pyquickbench.run_benchmark(\n    all_sizes_lin                   ,\n    all_funs                        ,\n    filename = timings_filename     ,\n    show = True                     ,\n)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}